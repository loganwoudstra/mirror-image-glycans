{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfeb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7ded44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>IUPAC</th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CFG-7-Sp8</td>\n",
       "      <td>Gal(α-Sp8</td>\n",
       "      <td>OC[C@@H](O1)[C@H](O)[C@H](O)[C@@H](O)[C@H]1-OC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CFG-8-Sp8</td>\n",
       "      <td>Glc(α-Sp8</td>\n",
       "      <td>OC[C@@H](O1)[C@@H](O)[C@H](O)[C@@H](O)[C@H]1-O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CFG-9-Sp8</td>\n",
       "      <td>Man(α-Sp8</td>\n",
       "      <td>OC[C@@H](O1)[C@@H](O)[C@H](O)[C@H](O)[C@H]1-OC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CFG-10-Sp8</td>\n",
       "      <td>GalNAc(α-Sp8</td>\n",
       "      <td>OC[C@@H](O1)[C@H](O)[C@H](O)[C@@H](NC(=O)C)[C@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CFG-10-Sp15</td>\n",
       "      <td>GalNAc(α-Sp15</td>\n",
       "      <td>OC[C@@H](O1)[C@H](O)[C@H](O)[C@@H](NC(=O)C)[C@...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name          IUPAC  \\\n",
       "0    CFG-7-Sp8      Gal(α-Sp8   \n",
       "1    CFG-8-Sp8      Glc(α-Sp8   \n",
       "2    CFG-9-Sp8      Man(α-Sp8   \n",
       "3   CFG-10-Sp8   GalNAc(α-Sp8   \n",
       "4  CFG-10-Sp15  GalNAc(α-Sp15   \n",
       "\n",
       "                                              SMILES  \n",
       "0  OC[C@@H](O1)[C@H](O)[C@H](O)[C@@H](O)[C@H]1-OC...  \n",
       "1  OC[C@@H](O1)[C@@H](O)[C@H](O)[C@@H](O)[C@H]1-O...  \n",
       "2  OC[C@@H](O1)[C@@H](O)[C@H](O)[C@H](O)[C@H]1-OC...  \n",
       "3  OC[C@@H](O1)[C@H](O)[C@H](O)[C@@H](NC(=O)C)[C@...  \n",
       "4  OC[C@@H](O1)[C@H](O)[C@H](O)[C@@H](NC(=O)C)[C@...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glycans_df = pd.read_csv(\"data/model_training/Glycans-CFG611.txt\", sep=\"\\t\")\n",
    "glycans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148666b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lectin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.203471</td>\n",
       "      <td>0.187753</td>\n",
       "      <td>-0.026359</td>\n",
       "      <td>0.016505</td>\n",
       "      <td>-0.040728</td>\n",
       "      <td>-0.044468</td>\n",
       "      <td>-0.053233</td>\n",
       "      <td>0.068222</td>\n",
       "      <td>-0.236435</td>\n",
       "      <td>-0.040679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120680</td>\n",
       "      <td>-0.032572</td>\n",
       "      <td>-0.136013</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>-0.630671</td>\n",
       "      <td>0.195180</td>\n",
       "      <td>0.179230</td>\n",
       "      <td>-0.054426</td>\n",
       "      <td>-0.042039</td>\n",
       "      <td>0.188738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.120131</td>\n",
       "      <td>0.135170</td>\n",
       "      <td>-0.082275</td>\n",
       "      <td>0.115452</td>\n",
       "      <td>-0.109133</td>\n",
       "      <td>-0.208142</td>\n",
       "      <td>0.072962</td>\n",
       "      <td>-0.011479</td>\n",
       "      <td>-0.066582</td>\n",
       "      <td>-0.065606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171328</td>\n",
       "      <td>-0.018229</td>\n",
       "      <td>-0.096660</td>\n",
       "      <td>0.098843</td>\n",
       "      <td>-1.036492</td>\n",
       "      <td>-0.017578</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>-0.047520</td>\n",
       "      <td>0.227151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055755</td>\n",
       "      <td>0.187735</td>\n",
       "      <td>0.031612</td>\n",
       "      <td>0.053205</td>\n",
       "      <td>-0.107346</td>\n",
       "      <td>-0.035376</td>\n",
       "      <td>-0.165884</td>\n",
       "      <td>0.151764</td>\n",
       "      <td>-0.107026</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271150</td>\n",
       "      <td>-0.127574</td>\n",
       "      <td>-0.048594</td>\n",
       "      <td>-0.009104</td>\n",
       "      <td>-0.588789</td>\n",
       "      <td>-0.120662</td>\n",
       "      <td>-0.159802</td>\n",
       "      <td>0.092370</td>\n",
       "      <td>0.170785</td>\n",
       "      <td>0.165332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051498</td>\n",
       "      <td>0.204848</td>\n",
       "      <td>-0.073019</td>\n",
       "      <td>0.087792</td>\n",
       "      <td>-0.116100</td>\n",
       "      <td>-0.054806</td>\n",
       "      <td>-0.131069</td>\n",
       "      <td>0.102224</td>\n",
       "      <td>-0.053619</td>\n",
       "      <td>0.029918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268785</td>\n",
       "      <td>-0.112583</td>\n",
       "      <td>-0.064447</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>-0.608291</td>\n",
       "      <td>-0.093020</td>\n",
       "      <td>-0.082011</td>\n",
       "      <td>0.045429</td>\n",
       "      <td>0.162284</td>\n",
       "      <td>0.119839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.047237</td>\n",
       "      <td>0.211114</td>\n",
       "      <td>0.141265</td>\n",
       "      <td>-0.020350</td>\n",
       "      <td>-0.106777</td>\n",
       "      <td>-0.230495</td>\n",
       "      <td>-0.077538</td>\n",
       "      <td>0.029638</td>\n",
       "      <td>-0.231603</td>\n",
       "      <td>-0.034559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351602</td>\n",
       "      <td>-0.005711</td>\n",
       "      <td>-0.153086</td>\n",
       "      <td>0.175410</td>\n",
       "      <td>-0.394899</td>\n",
       "      <td>0.075253</td>\n",
       "      <td>0.321678</td>\n",
       "      <td>-0.151691</td>\n",
       "      <td>-0.050353</td>\n",
       "      <td>0.194880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1         2         3         4         5         6         7  \\\n",
       "Lectin                                                                         \n",
       "1       0.203471  0.187753 -0.026359  0.016505 -0.040728 -0.044468 -0.053233   \n",
       "2       0.120131  0.135170 -0.082275  0.115452 -0.109133 -0.208142  0.072962   \n",
       "3       0.055755  0.187735  0.031612  0.053205 -0.107346 -0.035376 -0.165884   \n",
       "4       0.051498  0.204848 -0.073019  0.087792 -0.116100 -0.054806 -0.131069   \n",
       "5      -0.047237  0.211114  0.141265 -0.020350 -0.106777 -0.230495 -0.077538   \n",
       "\n",
       "               8         9        10  ...      1271      1272      1273  \\\n",
       "Lectin                                ...                                 \n",
       "1       0.068222 -0.236435 -0.040679  ...  0.120680 -0.032572 -0.136013   \n",
       "2      -0.011479 -0.066582 -0.065606  ...  0.171328 -0.018229 -0.096660   \n",
       "3       0.151764 -0.107026  0.022265  ...  0.271150 -0.127574 -0.048594   \n",
       "4       0.102224 -0.053619  0.029918  ...  0.268785 -0.112583 -0.064447   \n",
       "5       0.029638 -0.231603 -0.034559  ...  0.351602 -0.005711 -0.153086   \n",
       "\n",
       "            1274      1275      1276      1277      1278      1279      1280  \n",
       "Lectin                                                                        \n",
       "1       0.013133 -0.630671  0.195180  0.179230 -0.054426 -0.042039  0.188738  \n",
       "2       0.098843 -1.036492 -0.017578  0.013231  0.002841 -0.047520  0.227151  \n",
       "3      -0.009104 -0.588789 -0.120662 -0.159802  0.092370  0.170785  0.165332  \n",
       "4       0.004590 -0.608291 -0.093020 -0.082011  0.045429  0.162284  0.119839  \n",
       "5       0.175410 -0.394899  0.075253  0.321678 -0.151691 -0.050353  0.194880  \n",
       "\n",
       "[5 rows x 1280 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectin_esm_df = pd.read_csv(\"data/model_training/Protein-Feature-Table-2025-07-23.txt\", sep=\"\\t\", index_col=0)\n",
    "lectin_esm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f50158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glycan</th>\n",
       "      <th>Lectin</th>\n",
       "      <th>Concentration</th>\n",
       "      <th>Fraction_Bound</th>\n",
       "      <th>Fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CFG-7-Sp8</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CFG-8-Sp8</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CFG-9-Sp8</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CFG-10-Sp15</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CFG-10-Sp8</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Glycan  Lectin  Concentration  Fraction_Bound  Fold\n",
       "0    CFG-7-Sp8       1          200.0        0.000058     7\n",
       "1    CFG-8-Sp8       1          200.0        0.001217     1\n",
       "2    CFG-9-Sp8       1          200.0        0.000000     1\n",
       "3  CFG-10-Sp15       1          200.0        0.000723     8\n",
       "4   CFG-10-Sp8       1          200.0        0.000000     8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbound_df = pd.read_csv(\"data/model_training/Fraction_Bound.txt\", sep=\"\\t\")\n",
    "fbound_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2232dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morgan_fingerprint(smiles, radius, fpSize):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return torch.zeros(fpSize)\n",
    "\n",
    "    mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=fpSize, includeChirality=True)\n",
    "    fp = mfpgen.GetFingerprint(mol)  \n",
    "    return torch.tensor(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df9c0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# morgan fingerprint representation\n",
    "glycan_reps = {row.Name: get_morgan_fingerprint(row.SMILES, 3, 1024) for row in glycans_df.itertuples(index=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ee33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esm representation\n",
    "lectin_reps = {i: torch.tensor(row.values) for i, row in lectin_esm_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d8d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_len = len(list(glycan_reps.values())[0])\n",
    "esm_len = len(list(lectin_reps.values())[0])\n",
    "\n",
    "X = torch.zeros((len(fbound_df), mf_len + esm_len + 1), dtype=torch.float)\n",
    "Y = torch.tensor(fbound_df[\"Fraction_Bound\"], dtype=torch.float)\n",
    "folds = fbound_df[\"Fold\"]\n",
    "\n",
    "for i, row in enumerate(fbound_df.itertuples(index=False)):\n",
    "    glycan_rep = glycan_reps[row.Glycan].to(dtype=torch.float)\n",
    "    lectin_rep = lectin_reps[row.Lectin].to(dtype=torch.float)\n",
    "    conc = torch.tensor([row.Concentration], dtype=torch.float)\n",
    "    \n",
    "    X[i] = torch.cat([glycan_rep, lectin_rep, conc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bf0744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Linear(input_dim if i==0 else hidden_dims[i-1], hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(0.1)\n",
    "            ) for i, hidden_dim in enumerate(hidden_dims)],\n",
    "            nn.Linear(hidden_dims[-1], 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97d1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch_model(model_name, params, save_folder, folds):\n",
    "    torch.save(params, f\"models/{save_folder}/params.pt\")\n",
    "    num_folds = folds.nunique()\n",
    "    folds = torch.tensor(folds, device=X.device)\n",
    "    losses = []\n",
    "\n",
    "    # K-Fold Cross-Validation\n",
    "    for fold in range(num_folds):\n",
    "        print(f\"--- Fold {fold + 1}/{num_folds} ---\")\n",
    "\n",
    "        # Split data\n",
    "        test_mask = (folds == fold)\n",
    "        train_mask = ~test_mask\n",
    "\n",
    "        X_train, Y_train = X[train_mask], Y[train_mask]\n",
    "        X_test, Y_test = X[test_mask], Y[test_mask]\n",
    "\n",
    "        # Create Dataloaders\n",
    "        train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=params[\"batch_size\"])\n",
    "\n",
    "        # Initialize model, loss, optimizer\n",
    "        if model_name == \"mlp\":\n",
    "            model = MLP(X.shape[1], params[\"hidden_dims\"]).to(X.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(params[\"epochs\"]):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                total_test_loss = 0\n",
    "                for xb, yb in test_loader:\n",
    "                    preds = model(xb)\n",
    "                    loss = criterion(preds, yb)\n",
    "                    total_test_loss += loss.item() * xb.size(0)\n",
    "                avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{params['epochs']} - Train Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "        losses.append(avg_test_loss)\n",
    "        torch.save(model, f\"models/{save_folder}/fold-{fold}.pt\")\n",
    "    torch.save(losses, f\"models/{save_folder}/losses.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c93d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nontorch_model(model_name, params, save_folder, folds):\n",
    "    os.makedirs(f\"models/{save_folder}\", exist_ok=True)\n",
    "    torch.save(params, f\"models/{save_folder}/params.pt\")\n",
    "\n",
    "    num_folds = folds.nunique()\n",
    "    folds = torch.tensor(folds)\n",
    "    losses = []\n",
    "\n",
    "    # K-Fold Cross-Validation\n",
    "    for fold in range(num_folds):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{num_folds} ---\")\n",
    "\n",
    "        test_mask = (folds == fold)\n",
    "        train_mask = ~test_mask\n",
    "\n",
    "        X_train = X[train_mask].cpu().numpy()\n",
    "        Y_train = Y[train_mask].cpu().numpy()\n",
    "        X_test = X[test_mask].cpu().numpy()\n",
    "        Y_test = Y[test_mask].cpu().numpy()\n",
    "\n",
    "        # Train Model\n",
    "        if model_name == \"random_forest\":\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=params[\"n_estimators\"],\n",
    "                max_depth=params[\"max_depth\"],\n",
    "                random_state=params[\"random_state\"]\n",
    "            )\n",
    "        elif model_name == \"xgboost\":\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        preds = model.predict(X_test)\n",
    "        mse = mean_squared_error(Y_test, preds)\n",
    "        print(f\"Test MSE: {mse:.4f}\")\n",
    "        losses.append(mse)\n",
    "\n",
    "        # Save model\n",
    "        joblib.dump(model, f\"models/{save_folder}/fold-{fold}.joblib\")\n",
    "\n",
    "    # Save losses\n",
    "    torch.save(losses, f\"models/{save_folder}/losses.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "230d350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, params, save_folder):\n",
    "    # NOTE: idk why, but the variable 'folds' is not recognized within the follwoing functions, \n",
    "    # but is recognized within this function\n",
    "    if model_name in [\"mlp\"]:\n",
    "        train_torch_model(model_name, params, save_folder, folds)\n",
    "    elif model_name in [\"random_forest\", \"xgboost\"]:\n",
    "        train_nontorch_model(model_name, params, save_folder, folds)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb0196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = '256_nn'\n",
    "params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"hidden_dims\": [256],\n",
    "}\n",
    "train_model(\"mlp\", params, save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a20456",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = '128_nn'\n",
    "params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"hidden_dims\": [128],\n",
    "}\n",
    "train_model(\"mlp\", params, save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d5a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\AppData\\Local\\Temp\\ipykernel_5760\\1748222548.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  folds = torch.tensor(folds, device=X.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 1/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0042, Test Loss: 0.0030\n",
      "Epoch 2/5 - Train Loss: 0.0029, Test Loss: 0.0027\n",
      "Epoch 3/5 - Train Loss: 0.0027, Test Loss: 0.0029\n",
      "Epoch 4/5 - Train Loss: 0.0025, Test Loss: 0.0024\n",
      "Epoch 5/5 - Train Loss: 0.0024, Test Loss: 0.0022\n",
      "--- Fold 2/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0042, Test Loss: 0.0034\n",
      "Epoch 2/5 - Train Loss: 0.0029, Test Loss: 0.0024\n",
      "Epoch 3/5 - Train Loss: 0.0027, Test Loss: 0.0025\n",
      "Epoch 4/5 - Train Loss: 0.0026, Test Loss: 0.0023\n",
      "Epoch 5/5 - Train Loss: 0.0024, Test Loss: 0.0020\n",
      "--- Fold 3/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0041, Test Loss: 0.0032\n",
      "Epoch 2/5 - Train Loss: 0.0028, Test Loss: 0.0029\n",
      "Epoch 3/5 - Train Loss: 0.0026, Test Loss: 0.0028\n",
      "Epoch 4/5 - Train Loss: 0.0025, Test Loss: 0.0025\n",
      "Epoch 5/5 - Train Loss: 0.0024, Test Loss: 0.0024\n",
      "--- Fold 4/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0046, Test Loss: 0.0024\n",
      "Epoch 2/5 - Train Loss: 0.0029, Test Loss: 0.0023\n",
      "Epoch 3/5 - Train Loss: 0.0027, Test Loss: 0.0023\n",
      "Epoch 4/5 - Train Loss: 0.0025, Test Loss: 0.0022\n",
      "Epoch 5/5 - Train Loss: 0.0024, Test Loss: 0.0020\n",
      "--- Fold 5/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0046, Test Loss: 0.0027\n",
      "Epoch 2/5 - Train Loss: 0.0028, Test Loss: 0.0025\n",
      "Epoch 3/5 - Train Loss: 0.0026, Test Loss: 0.0022\n",
      "Epoch 4/5 - Train Loss: 0.0025, Test Loss: 0.0022\n",
      "Epoch 5/5 - Train Loss: 0.0024, Test Loss: 0.0022\n",
      "--- Fold 6/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0041, Test Loss: 0.0044\n",
      "Epoch 2/5 - Train Loss: 0.0027, Test Loss: 0.0045\n",
      "Epoch 3/5 - Train Loss: 0.0025, Test Loss: 0.0038\n",
      "Epoch 4/5 - Train Loss: 0.0024, Test Loss: 0.0039\n",
      "Epoch 5/5 - Train Loss: 0.0023, Test Loss: 0.0036\n",
      "--- Fold 7/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0042, Test Loss: 0.0024\n",
      "Epoch 2/5 - Train Loss: 0.0029, Test Loss: 0.0026\n",
      "Epoch 3/5 - Train Loss: 0.0027, Test Loss: 0.0022\n",
      "Epoch 4/5 - Train Loss: 0.0025, Test Loss: 0.0020\n",
      "Epoch 5/5 - Train Loss: 0.0024, Test Loss: 0.0020\n",
      "--- Fold 8/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0042, Test Loss: 0.0030\n",
      "Epoch 2/5 - Train Loss: 0.0028, Test Loss: 0.0029\n",
      "Epoch 3/5 - Train Loss: 0.0027, Test Loss: 0.0026\n",
      "Epoch 4/5 - Train Loss: 0.0025, Test Loss: 0.0024\n",
      "Epoch 5/5 - Train Loss: 0.0024, Test Loss: 0.0023\n",
      "--- Fold 9/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0042, Test Loss: 0.0030\n",
      "Epoch 2/5 - Train Loss: 0.0029, Test Loss: 0.0028\n",
      "Epoch 3/5 - Train Loss: 0.0027, Test Loss: 0.0027\n",
      "Epoch 4/5 - Train Loss: 0.0025, Test Loss: 0.0026\n",
      "Epoch 5/5 - Train Loss: 0.0024, Test Loss: 0.0024\n",
      "--- Fold 10/10 ---\n",
      "Epoch 1/5 - Train Loss: 0.0042, Test Loss: 0.0029\n",
      "Epoch 2/5 - Train Loss: 0.0029, Test Loss: 0.0027\n",
      "Epoch 3/5 - Train Loss: 0.0027, Test Loss: 0.0025\n",
      "Epoch 4/5 - Train Loss: 0.0025, Test Loss: 0.0023\n",
      "Epoch 5/5 - Train Loss: 0.0023, Test Loss: 0.0021\n"
     ]
    }
   ],
   "source": [
    "save_folder = '512_nn'\n",
    "params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"hidden_dims\": [512],\n",
    "}\n",
    "train_model(\"mlp\", params, save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd169859",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = '512_256_nn'\n",
    "params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"hidden_dims\": [512, 256],\n",
    "}\n",
    "train_model(\"mlp\", params, save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ebeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/10 ---\n"
     ]
    }
   ],
   "source": [
    "save_folder = 'rf_model'\n",
    "params = {\n",
    "    \"n_estimators\": 25,\n",
    "    \"max_depth\": 6,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "train_model(\"random_forest\", params, save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = 'xgb_model'\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 50,\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"verbosity\": 1\n",
    "}\n",
    "train_model(\"xgboost\", params, save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6037cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\AppData\\Local\\Temp\\ipykernel_22356\\2904179942.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  losses = torch.load(pt_path)\n"
     ]
    }
   ],
   "source": [
    "all_losses = {}\n",
    "\n",
    "for save_folder in os.listdir(\"models\"):\n",
    "    folder_path = os.path.join(\"models\", save_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    pt_path = os.path.join(folder_path, \"losses.pt\")\n",
    "    npy_path = os.path.join(folder_path, \"losses.npy\")\n",
    "\n",
    "    if os.path.isfile(pt_path):\n",
    "        losses = torch.load(pt_path)\n",
    "    elif os.path.isfile(npy_path):\n",
    "        losses = np.load(npy_path).tolist()\n",
    "    else:\n",
    "        print(f\"No losses file found in {save_folder}\")\n",
    "        continue\n",
    "\n",
    "    all_losses[save_folder] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e83fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_nn 0.0024162003398329733\n",
      "256_nn 0.0024528425573110833\n",
      "512_256_nn 0.0023599095073404096\n",
      "512_nn 0.0023166615587496475\n",
      "rf_model 0.0026659936915438775\n",
      "xgb_model 0.0020314463414251803\n"
     ]
    }
   ],
   "source": [
    "for model, losses in all_losses.items():\n",
    "    print(model, np.mean(losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glycan-env-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
